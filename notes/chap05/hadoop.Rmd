A Primer on MapReduce and Hadoop
--------------------------------

Hadoop is an open-source framework for large-scale data storage and distributed computing, built on the MapReduce model. Doug Cutting created Hadoop as a component of the Nutch web crawler. It became its own project in 2006, and graduated to a top-level Apache project in 2008.

It is a general framework, applicable to a variety of domains and programming languages. One use case is to drive large R jobs.

## Hadoop at Cruising Altitude

Hadoop is a framework for parallel processing:  
* decompose a problem into independent units of work;  
* distribute that work across a cluster of machines.  

Use for extract-transform-load (ETL), image processing, data analysis, etc. It is useful for big data and compute intensive tasks.

Hadoop’s parallelism is based on the *MapReduce model*.

## A MapReduce Primer

The MapReduce model outlines a way to perform work across a cluster built of inex- pensive, commodity machines.

Two phases:  
* Map phase: divide that input and group the pieces into smaller, independent piles of related material;   
* Reduce phase: perform some action on each pile.   

MapReduce is a “divide-and-conquer” model. The piles can be Reduced in parallel because they do not rely on one another.

Map Phase:   
1. Each cluster node runs a part of the initial big data and runs a Map task on each record (item) of input.  
2. The Map tasks run in parallel and creates a *key/value* pair for each record. The key identifies the items pile for the reduce operation. The value is often the record itself.  

The Shuffle:  
Each key/value pair is assigned a pile based on the key.

Reduce Phase:  
1. The cluster nodes then run the Reduce task on each pile.  
2. The Reduce task typically emits output for each pile.  

See Figure 5.1.

## Thinking in MapReduce: Some Pseudocode Examples

For these examples, I’ll use a fictitious text input format in which each record is a comma-separated line that describes a phone call:  

  {date}, {caller num}, {caller carrier}, {dest num}, {dest carrier}, {length}

### Calculate Average Call Length for Each Date

Map task groups the records by day, and then calculates the mean (average) call length in the Reduce task.

Map task:  
* Receives a single line of input (that is, one input record)  
* Uses text manipulation to extract the {date} and {length} fields  
* Emits key: {date}, value: {length}  

Reduce task  
* Receives key:{date},values:{length1 ... lengthN} i.e., ,each reduce task receives all of the call lengths for a single date)  
* Loops through {length1 ... lengthN} to calculate total call length, and also to note the number of calls  
* Calculates the mean (divides the total call length by the number of calls)  
* Outputs the date and the mean call length  

### Number of Calls by Each User, on Each Date

The goal is to get a breakdown of each caller for each date. The Map phase will define the keys to group the inputs, and the Reduce task will perform the calculations. Notice that the Map task emits a dummy value (the number 1) as its value because we use the Reduce task for a simple counting operation.

Map task  
* Receives single line of input  
* Uses text manipulation to extract {date}, {caller num}  
* Emits key: {date}{caller num}, value: 1  

Reduce task  
* Receives key: {date}{caller num}, value: {1 ... 1}  
* Loops through each item, to count total number of items (calls)  
* Outputs {date}, {caller num} and the number of calls  

### Run a Special Algorithm on Each Record

No need to group the input records; just run some special function for every input record. Leverage MapReduce to execute some (possibly long-running) code for each input record and reap the time-saving parallel execution.

Map task  
* Receives single line of input  
* Uses text manipulation to extract function parameters  
* Passes those parameters to a potentially long-running function  
* Emits key: {function output}, value: {null}  

No Reduce task.

## Binary and Whole-File Data: SequenceFiles

You’re in a different situation if you plan to use Hadoop with binary data (sound files, image files, proprietary data formats) or if you want to treat an entire text file (XML document) as a record.

By default, when you point Hadoop to an input file, it will assume it is a text document and treat each line as a record. There are times when this is not what you want: maybe you’re performing feature extraction on sound files, or you wish to perform sentiment analysis on text documents. 

Use a special archive called a *SequenceFile*. A SequenceFile is similar to a zip or tar file, in that it’s just a container for other files. Hadoop considers each file in a SequenceFile to be its own record.

To manage zip files, you use the `zip` command. Tar file? Use `tar`. SequenceFiles? Hadoop doesn’t ship with any tools for this, but you still have options: you can write a Hadoop job using the Java API; or you can use the `forqlift` command-line tool. 

`forqlift` strives to be simple and straightforward. For example, to create a SequenceFile from a set of MP3s, you would run:
```
forqlift create --file=/path/to/file.seq *.mp3
```  
Then, in a Hadoop job, the Map task’s key would be an MP3’s filename and the value would be the file’s contents.

## No Cluster? No Problem! Look to the Clouds...

A Hadoop cluster can be built locally or using a cloud service, e.g., Amazon Web Service (AWS). 

AWS provides computing resources such as virtual servers and storage in *metered* (pay- per-use) way. You can hand-build your cluster using virtual servers on Elastic Compute Cloud (EC2), or you can leverage the Hadoop-on-demand service called Elastic MapReduce (EMR).

An EMR-based cluster is designed to be ephemeral: by default, AWS tears down the cluster as soon as your job completes. All of the cluster nodes and resources disappear. That means you can’t leverage HDFS for long-term storage.



