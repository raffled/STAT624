---
title: "Division and Recombination"
author: "jharner"
date: "March 1, 2015"
output: html_document
---

## High-Level Interface

`datadr` provides a high-level language for D&R that simply consists of functions `divide()` for performing division, and `recombine()` for performing recombinations. The goal is for these methods to be sufficient for most operations a user might want to carry out.

`divide()` and `recombine()` provide a way to create a persistent partitioning of the data in various ways and then provide several mechanisms combining results of analytical methods applied to the divisions. Being able to easily perform these operations alone provides a lot of power for ad-hoc analysis of very large data sets. However, we plan to inject results from D&R theory and methods to provide an even more rich environment for analysis.

See the figure.

## Divison

Division is achieved through the `divide()` method.

Currently there are two types of divisions supported: conditioning variable, and random replicate. In this section we discuss the major arguments to `divide()`, the most important of which is `by`.

### Conditioning variable division

In the previous section, we were looking at a division of the `iris` data by `species`. We manually split the data into key-value pairs. We can achieve the same result by doing conditioning variable division:
```{r}
library(datadr)
irisDdf <- ddf(iris)
# divide irisDdf by species
bySpecies <- divide(irisDdf, by = "Species", update = TRUE)
```

`divide()` must take a ddf object.

Since the result of splitting the `iris` data by `species` is a data frame, `bySpecies` is now a ddf. We can inspect it with the following:
```{r}
bySpecies
```

We see the same printout as we had with our manually-created division, with the addition of information about how the data was divided.

Conditioning variable division was specified with the `by` argument. Here, simply specifying a character string or vector of character strings (for multiple conditioning variables) will invoke conditioning variable division. A more formal way to achieve this is by using `condDiv()` to build the division specification:
```{r}
# divide irisDdf by species using condDiv()
bySpecies <- divide(irisDdf, by = condDiv("Species"), update = TRUE)
```

Using `condDiv()` is not necessary but follows the general idea of using a function to build a division specification that is and will be followed for other division methods.

Here's what a subset of the divide data looks like:
```{r}
# look at a subset of bySpecies
bySpecies[[1]]
```

Note that the `Species` column is missing in the value data frame. This is because it is the variable we split on, and therefore has the same value for the entire subset. All conditioning variables for a given subset are stored in a `splitVars` attribute, and can be retrieved by `getSplitVars()`:
```{r}
# get the split variable (Species) for some subsets
getSplitVars(bySpecies[[1]])
getSplitVars(bySpecies[[2]])
```

The keys for the division result are strings that specify how the data was divided:
```{r}
# look at bySpecies keys
getKeys(bySpecies)
```

### Random replicate division

Another way to divide data that is currently implemented is random replicate division. For this, we use the division specification function `rrDiv()`. This function allows you to specify the number of rows you would like each random subset to have, and optionally a random seed to use for the random assignment of rows to subsets.

Suppose we want to split the iris data into random subsets with roughly 10 rows per subset:
```{r}
# divide iris data into random subsets of 10 rows per subset
set.seed(123)
byRandom <- divide(bySpecies, by = rrDiv(10), update = TRUE)
```

Note that we passed `bySpecies` as the input data. We could just as well have specified `irisDdf` or any other division of the `iris` data. The input partitioning doesn't matter.
```{r}
byRandom
```

We see there are still 150 rows (as there should be), but now there are 15 subsets.

We can look at the distribution of the of the number of rows in each subset:
```{r}
library(ggplot2)
# plot distribution of the number of rows in each subset
qplot(y = splitRowDistn(byRandom),
   xlab = "percentile", ylab = "number of rows in subset")
```

We see that there are not exactly 10 rows in each subset, but 10 rows on average. The random replicate algorithm simply randomly assigns each row of the input data into the number of bins $K$ determined by the total number of rows $n$ in the data divided by the desired number of rows per subset. Thus the distribution of the number of rows in each subset is like a draw from a multinomial with number of trials $n$ and event probabilities of being put into one of $K$ bins as $p_i = \frac{1}{K} ,i = 1, \ldots , K$. We are working on a scalable approach to randomly assign exactly $n/K$ rows to each subset.

The keys for random replicate divided data are simply labels indicating the bin:
```{r}
head(getKeys(byRandom))
```

### Using `addTransform()` with `divide()`

`divide()` does not know how to break data into pieces unless it is dealing with data frames. But sometimes we have input data that is not a ddf, or sometimes we would like to transform a ddf to add new columns before performing the division. We can use `addTransform()` to get inputs suitable for the division result we would like to achieve.

### Using `addTransform()` to create a derived conditioning variable

A common use of `addTransform()` when the input data is a ddf is to create a derived variable upon which we will perform division. For example, suppose we would like to divide the `iris` data by both `Species` and a discretized version of `Sepal.Length`.

First, let's get a feel for the range of the `Sepal.Length` variable:
```{r}
summary(bySpecies)$Sepal.Length$range
```

We see that its range is from 4.3 to 7.9. Suppose we want to bin `Sepal.Length` by the integer. We can create a new variable `slCut` by adding a transformation to the data that adds this column to the data frame in each subset.
```{r}
irisDdfSlCut <- addTransform(irisDdf, function(v) {
   v$slCut <- cut(v$Sepal.Length, seq(0, 8, by = 1))
   v
})
irisDdfSlCut[[1]]
```

We see that `irisDdfSlCut` has the new variable slCut, as we expect. Now we can pass this to divide and split by both `Species` and `slCut`:
```{r}
# divide on Species and slCut
bySpeciesSL <- divide(irisDdfSlCut, by = c("Species", "slCut"))
```

Let's look at one subset:
```{r}
bySpeciesSL[[3]]
```

As the key indicates, the species for this subset is `versicolor` and the sepal length is in the range `(4,5]`. Recall that we can access the split variables for this subset with:
```{r}
getSplitVars(bySpeciesSL[[3]])
```

### The `postTransFn` argument

`postTransFn` provides a way for you to change the structure of the data after division, but prior to it being written to disk. This can be used to get the data out of data frame mode or to subset or remove columns, etc. It is specified in a way similar to `addTransform()`, where if it has two arguments it will be passed the `key` and the `value` and if it has one argument it will be passed the `key`.

Since the input to `divide()` is a ddf, the `postTransFn` function will be receiving values which are some subset of that data frame, so you know what type of data to anticipate in the function, and you can test it on input key-value pairs to your call to `divide()`.

### The `spill` argument

Many times a conditioning variable division of interest will result in a long-tailed distribution of the data belonging to each subset, such that the data going into some subsets will get too large (remember that each subset must be small enough to be processed efficiently in memory). The `spill` argument in `divide()` allows you to specify a limit to the number of rows that can belong in a subset, after which additional records will get "spilled" into a new subset.

For example, suppose we want no more than 12 rows per subset in our by-species division:
```{r}
# divide iris data by species, spilling to new key-value after 12 rows
bySpeciesSpill <- divide(irisDdf, by = "Species", spill = 12, update = TRUE)
```

Let's see what our subsets look like now:
```{r}
# look at some subsets
bySpeciesSpill[[1]]
bySpeciesSpill[[5]]
```

There are 5 different subsets for each species. For example, "Species=setosa" has subset with keys: `Species=setosa_1`, ..., `Species=setosa_5`. The first four subsets have 12 rows in each (each spilling into a new subset after it was filled with 12 rows), and the fifth subset has 2 rows, a total of 50 rows for `Species=setosa`.

### The `filter` argument

The `filter` argument to `divide()` is an optional function that is applied to each candidate post-division key-value pair to determine whether it should be part of the resulting division. A common case of when the `filter` argument is useful is when a division may result in a very large number of very small subsets and we are only interested in studying subsets with adequate size.

As an example, consider the `iris` splitting with `spill = 12` from before. Suppose that in addition to spilling records, we also only want to keep subsets that have more than 5 records in them.
```{r}
# divide iris data by species, spill, and filter out subsets with <=5 rows
bySpeciesFilter <- divide(irisDdf, by = "Species", spill = 12,
   filter = function(v) nrow(v) > 5, update = TRUE)
bySpeciesFilter
```

The `filter` function simply returns `TRUE` if we want to keep the subset and `FALSE` if not.

Now we have 144 rows and 12 divisions - the 3 subsets with 2 rows were omitted from the result.

Note that the `filter` is applied to the data prior to the application of `postTransFn`. Thus your `filter` function can expect the same structure of data frame as is in the values of your input ddf.

## Recombination

In this section we cover basic usage of the recombine() method.

We will show some examples on the `iris` data divided by species.
```{r}
irisDdf <- ddf(iris)
bySpecies <- divide(irisDdf, by = "Species", update = TRUE)
```

Recall that in D&R we specify a data division, apply a number of numeric or visual methods to each subset of the division, and then recombine the results of those computations. Typically the application of the analytic method and the recombination go hand-in-hand -- a ddo/ddf is typically transformed with `addTransform()` prior to applying `recombine()`.

### `combine` argument

Aside from specifying the input `data` ddo/ddf object, the main argument in `recombine()` is `combine`, which specifies the recombination strategy. There are several options for combine built in to `datadr`, and new ones can be specified. They come in a few categories:  

* combiners that pull results into local R session  
* combiners that return a new ddo/ddf  
* combiners that compute statistics

#### Combiners that pull results into local R session  

Often the analytical method we apply to each subset results in a small enough result that we can pull all of the results together into our local R session. This is one of the more frequently-used recombination strategies. For this, there are currently two `combine` options:  

* `combCollect`: (the default) - returns a list of key-value pairs  
* `combRbind`: rbinds all of the values into a single data frame  

Suppose we would like to compute the mean petal width for each species in our `bySpecies` division and pull the result back into our R session as a list of key-value pairs:
```{r}
# apply mean petal width transformation
mpw <- addTransform(bySpecies, function(v) mean(v$Petal.Width))
# recombine using the default combine=combCollect
recombine(mpw)
```

Here, the default `combCollect` was used to combine the results, giving us a list of key-value pairs with the value being the mean petal width.

If we would like the result to be a data frame we can use `combine=combRbind`:
```{r}
recombine(mpw, combRbind)
```

The scalar mean is coerced into a data frame. Note that by default if the input data keys are characters, they will be added to the data frame.

#### Combiners that return a new ddo/ddf

Sometimes we have applied a transformation to a ddo/ddf and want the result to be a new ddo/ddf object with the transformation permanently applied. We might want to do this:

* to have a smaller data set to work with for further D&R operations;    
* to make the result a new persistent data object to avoid future recomputations of the transformation.  

For this type of recombination, we have two options for the `combine` argument:  

* `combDdo`: persist the data as a ddo  
* `combDdf`: persist the data as a ddf  

For example, if I want the mean petal width transformation to persist as a ddo:
```{r}
recombine(mpw, combDdo)
```

#### Combiners that compute statistics

There are also some experimental recombination strategies that not only pull computational results together, but also merge the results in some statistical way.

The current methods for doing this in `datadr` are very experimental and mostly useful for illustrative purposes of what can be done. We will cover two examples:

* `combMean`: for transformations that return a vector, return the element-wise means  
* `combMeanCoef`: for transformations that return model coefficients, average the coefficients  

Much of the anticipated future work for `datadr` is the construction of several `apply-combine` pairs that are useful for different analysis tasks. The apply/combine pairs `drGLM()`-`combMeanCoef()` and `drBLB()`-`combMeanCoef()` that we will show later are two initial examples.

## D&R Examples

Here are some examples with a new (but still small) data set that illustrate some general use of division and recombination including the use of random replicate division and some different recombination methods to fit a GLM to a dataset.

Although there are different approaches for in-memory data like this one, we will use `datadr` tools to deal with the data throughout, again remembering that these tools scale.

### The data

The data is adult income from the 1994 census database, pulled from the UCI machine learning repository.

First, we load the data and turn it into a ddf:
```{r}
data(adult)
# turn adult into a ddf
adultDdf <- ddf(adult, update = TRUE)
adultDdf
# look at the names
names(adultDdf)
```

We see that there are about 32K observations, and we see the various variables available.

We'll start with some simple exploratory analysis. One variable of interest in the data is education. We can look at the summary statistics to see the frequency distribution of `education` (which were computed since we specified `update = TRUE` when we created `adultDdf`):
```{r}
library(lattice)
edTable <- summary(adultDdf)$education$freqTable
edTable$value <- with(edTable, reorder(value, Freq, mean))
dotplot(value ~ Freq, data = edTable)
```

### Division by education group

Perhaps we would like to divide our data by `education` and investigate how some of the other variables behave within education.

Suppose we want to make some changes to the `education` variable: we want to leave out "Preschool" and create groups "Some-elementary", "Some-middle", and "Some-HS". Of course in a real analysis you would probably want to first make sure you aren't washing any interesting effects out by making these groupings.

We can handle these changes to the `education` variable using `preTransFn` in our call to `divide()`. You might be wondering why not make the changes to the variable in the original data frame prior to doing all of this. For this example, of course we can do that, but suppose this data were, say, 1TB in size. You would probably much rather apply the transformation during the division than create a new set of data.

The following transformation function will achieve the desired result:
```{r}
# make a transformation to group some education levels
edGroups <- function(v) {
   v$edGroup <- as.character(v$education)
   v$edGroup[v$edGroup %in% c("1st-4th", "5th-6th")] <- "Some-elementary"
   v$edGroup[v$edGroup %in% c("7th-8th", "9th")] <- "Some-middle"
   v$edGroup[v$edGroup %in% c("10th", "11th", "12th")] <- "Some-HS"
   v
}
# test it
adultDdfGroup <- addTransform(adultDdf, edGroups)
adultDdfGroup[[1]]
```

This adds a variable `edGroup` with the desired grouping of education levels. We can now divide the data by `edGroup`. We specify a `filterFn` to only allow data to be output that does not correspond to "Preschool".
```{r}
# divide by edGroup and filter out "Preschool"
byEdGroup <- divide(adultDdfGroup, by = "edGroup", 
   filterFn = function(x) x$edGroup[1] != "Preschool",
   update = TRUE)
byEdGroup
```

We can look at the distribution of number of people in each education group with the following simple recombination:

We need to add a transformation to `byEdGroup` that simply compute the number of rows, and then use a `combRbind` recombine to collect all of the results in a single data frame:
```{r}
# add transformation to count number of people in each education group
byEdGroupNrow <- addTransform(byEdGroup, function(x) nrow(x))
# recombine into a data frame
edGroupTable <- recombine(byEdGroupNrow, combRbind)
edGroupTable
```

### Investigating data by education group

There are many things we might be interested in doing with our `byEdGroup` division. We'll just show one quick example.

One thing we might be interested in is how different the distribution of `gender` is within each of the `education` groups. One way to do this is to look at the ratio of men to women. We can compute this ratio by applying a simple transformation and a `combRbind` recombination:
```{r}
# compute male/female ratio by education group
byEdGroupSR <- addTransform(byEdGroup, function(x) {
   tab <- table(x$sex)
   data.frame(maleFemaleRatio = tab["Male"] / tab["Female"])
})
sexRatio <- recombine(byEdGroupSR, combRbind)
sexRatio
```

We can visualize it with the following:
```{r}
# make dotplot of male/female ratio by education group
sexRatio$edGroup <- with(sexRatio, reorder(edGroup, maleFemaleRatio, mean))
dotplot(edGroup ~ maleFemaleRatio, data = sexRatio)
```

We know the marginal distribution of `gender` is lopsided to begin with (see `summary(byEdGroup)$sex`), but we don't know if the sample we are dealing with is biased or not... There are obviously many many directions to go with the exploratory analysis and hopefully these few examples provide a start and a feel for how to go about

One more thing to note about what we have done so far: We have shown a couple of examples of using `datadr` to summarize the data in different ways and visualize the summaries. This is a good thing to do. But we also want to be able to visualize the subsets in detail. For example, we might want to look at a scatterplot of `age` vs. `hoursperweek`. With this small data set, we obviously can pull all subsets in and make a `lattice` plot or faceted `ggplot`. However, what if there are thousands or hundreds of thousands of subsets? This is where the `trelliscope` package -- a visualization companion to `datadr` -- comes in.

### Fitting a GLM to the data

Although the majority of the work we do is quite effective through clever use of generic division and recombination approaches and making heavy use of visualization, it is worthwhile to show some of the approaches of approximating all-data estimates with `datadr`.

Therefore, we now turn to some examples of ways to apply analytical methods across the entire dataset from within the D&R paradigm. For example, suppose we would like to model the dependence of making more or less than 50K per year on `educationnum`, `hoursperweek`, and `sex` using logistic regression.

Before doing it with  `datadr`, let's first apply the method to the original data frame, so that we can compare the results. Recall again that since this is a small data set, we can do things the "usual" way:
```{r}
# fit a glm to the original adult data frame
rglm <- glm(incomebin ~ educationnum + hoursperweek + sex, data = adult,
            family = binomial())
summary(rglm)$coefficients
```

Now let's compare this to a few `datadr` approaches. Note that these approaches are currently proof-of-concept only and are meant to illustrate ideas. We will illustrate `drGLM()` and `drBLB()`.

### Fitting a GLM with `drGLM()`

For the results of `drGLM()` and `drBLB()` to be valid, we need a random-replicate division of the data. We will choose a division that provides about 1000 rows in each subset and that only has the variables that we care about:
```{r}
rrAdult <- divide(adultDdf, by = rrDiv(1000), update = TRUE,
                  postTransFn = function(x) 
                    x[,c("incomebin", "educationnum", "hoursperweek", "sex")])
```

Now, we can apply a `drGLM()` transformation to `rrAdult` and then call `recombine()` on the result. `drGLM()` has been designed to take any arguments you might pass to `glm()` and apply it to each subset, doing some special manipulation to the results to work with the desired recombination, `combMeanCoef`, which is a function that has been designed specifically to take coefficient results from model fits applied to each subset and average them:
```{r}
adultGlm <- addTransform(rrAdult, function(x) 
  drGLM(incomebin ~ educationnum + hoursperweek + sex, 
        data = x, family = binomial()))
recombine(adultGlm, combMeanCoef)
```

If we compare the result to the all-data estimate, the values are close. However, with this approach, we do not get any inference about the estimates.

### Fitting a GLM with `drBLB()`

We can use the bag of little bootstraps (BLB) approach to fit a GLM to the data. The idea of bag of little bootstraps is to split the data into random subsets and apply a bootstrap method to each subset, compute a bootstrap metric to the result, and then average the metric across all subsets.

One important thing to keep in mind is that BLB requires each subset be resampled with with $N$ replications, $N$ being the total number of rows in the entire data set. Since each subset has much fewer than $N$ rows, say $n$, we can imitate taking $N$ draws by sampling from a multinomial with $n$ bins with uniform probability and assigning weights to each of the $n$ observations in the subset and computing weights from these and passing that as the `weights` argument to $glm()$. Any R method that meets BLB requirements and accommodates this sampling scheme in one way or another can be used with `drBLB()`.

We apply `drBLB()` to each subset, specifying the `statistic` to be computed for each bootstrap sample, the `metric` to compute on the statistics, and the number of bootstrap replications `R`. We also need to tell it the total number of rows in the data set. Right now, `drBLB()` simply returns a numeric vector, which is combined using `combMean()`.
```{r}
# add bag of little bootstraps transformation
adultBlb <- addTransform(rrAdult, function(x) {
   drBLB(x, 
      statistic = function(x, weights)
         coef(glm(incomebin ~ educationnum + hoursperweek + sex, 
            data = x, weights = weights, family = binomial())),
      metric = function(x)
         quantile(x, c(0.05, 0.95)),
      R = 100,
      n = nrow(rrAdult)
   )
})
# compute the mean of the resulting CI limits
coefs <- recombine(adultBlb, combMean)
matrix(coefs, ncol = 2, byrow = TRUE)
```

The result here is simply a vector, where each successive pair of elements represents the lower and upper 95% confidence limit for `intercept`, `educationnum`, `hoursperweek`, and `sexMale`. We recast the result to print it as a matrix. Close inspection shows that the confidence limits are similar to what is returned from the all-data `glm()` estimate and that confidence interval widths are about the same.


















